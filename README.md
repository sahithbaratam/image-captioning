# ğŸ–¼ï¸ AI Image Caption Generator with Voice Narration

An intelligent image captioning system that generates human-like descriptions for images using BLIP (Bootstrapped Language-Image Pre-training) model, enhanced with grammar polishing, multi-language translation, and text-to-speech capabilities.

## âœ¨ Features

- **ğŸ¤– AI-Powered Captioning**: Uses Salesforce's BLIP model for accurate image description
- **ğŸ“ Grammar Enhancement**: Automatically polishes captions using T5 paraphrasing model
- **ğŸŒ Multi-Language Support**: Translates captions to Telugu, Hindi, French, and Spanish
- **ğŸ¤ Voice Narration**: Generates audio narration using Google Text-to-Speech
- **ğŸ¨ Interactive UI**: User-friendly Gradio interface for easy interaction
- **âš¡ GPU Acceleration**: Optimized for CUDA-enabled environments

## ğŸ› ï¸ Installation

### Prerequisites
- Python 3.7+
- CUDA-compatible GPU (recommended)
- Google Colab or local Python environment

### Install Dependencies

```bash
pip install transformers torch torchvision pillow gradio
pip install gTTS deep-translator
```

## ğŸš€ Usage

### 1. Basic Usage (Programmatic)

```python
from transformers import BlipProcessor, BlipForConditionalGeneration
from PIL import Image
import torch

# Load model
device = "cuda" if torch.cuda.is_available() else "cpu"
processor = BlipProcessor.from_pretrained('Salesforce/blip-image-captioning-base')
model = BlipForConditionalGeneration.from_pretrained('Salesforce/blip-image-captioning-base').to(device)

# Generate caption
def generate_caption(image):
    inputs = processor(images=image, return_tensors="pt").to(device)
    output = model.generate(**inputs)
    caption = processor.decode(output[0], skip_special_tokens=True)
    return caption

# Load and process image
image = Image.open("your_image.jpg")
caption = generate_caption(image)
print(f"Caption: {caption}")
```

### 2. Interactive UI

Launch the Gradio interface:

```python
import gradio as gr

demo = gr.Interface(
    fn=caption_image,
    inputs=[
        gr.Image(type="pil", label="Upload an Image"),
        gr.Dropdown(["Telugu", "Hindi", "French", "Spanish"], label="Select Translation Language")
    ],
    outputs=[
        gr.Text(label="Raw Caption"),
        gr.Text(label="Polished Caption"),
        gr.Text(label="Translated Caption"),
        gr.Audio(label="Voice Narration (TTS)")
    ],
    title="ğŸ–¼ï¸ AI Image Captioning with ğŸ¤ Voice Narration"
)

demo.launch()
```

### 3. Google Colab

Open the notebook in Google Colab and run all cells. The interface will be automatically launched with a public URL.

## ğŸ“ Project Structure

```
ai-image-captioning/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ image_captioning.ipynb
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ captioning.py
â”‚   â”œâ”€â”€ translation.py
â”‚   â”œâ”€â”€ tts.py
â”‚   â””â”€â”€ ui.py
â””â”€â”€ examples/
    â”œâ”€â”€ sample_images/
    â””â”€â”€ outputs/
```

## ğŸ§  How It Works

1. **Image Processing**: The input image is processed using BLIP's image processor
2. **Caption Generation**: BLIP model generates an initial caption
3. **Grammar Enhancement**: T5 paraphrasing model polishes the caption
4. **Translation**: Google Translator converts the caption to selected language
5. **Voice Synthesis**: gTTS generates audio narration in English
6. **Output**: Returns raw caption, polished caption, translation, and audio

## ğŸ”§ Configuration

### Supported Languages
- Telugu (te)
- Hindi (hi)
- French (fr)
- Spanish (es)

### Model Parameters
- **BLIP Model**: `Salesforce/blip-image-captioning-base`
- **Grammar Model**: `Vamsi/T5_Paraphrase_Paws`
- **Translation**: Google Translator API
- **TTS**: Google Text-to-Speech

## ğŸ“Š Performance

- **GPU Memory**: ~2-4GB VRAM required
- **Processing Time**: 2-5 seconds per image (GPU)
- **Supported Formats**: JPEG, PNG, WebP, BMP
- **Max Image Size**: 1024x1024 pixels (recommended)

## ğŸ¯ Examples

### Input Image
![Sample Image](examples/sample_images/kids_playing.jpg)

### Output
- **Raw Caption**: "two boys fighting each other"
- **Polished Caption**: "Two boys fighting each other"
- **Telugu Translation**: "à°‡à°¦à±à°¦à°°à± à°•à±à°°à±à°°à°¾à°³à±à°³à± à°’à°•à°°à°¿à°¤à±‹ à°’à°•à°°à± à°ªà±‹à°°à°¾à°¡à±à°¤à±à°¨à±à°¨à°¾à°°à±"
- **Audio**: Generated TTS file

## ğŸš¨ Limitations

- Model may struggle with complex or abstract images
- Translation quality depends on Google Translator
- TTS currently supports English narration only
- GPU recommended for optimal performance

## ğŸ›¡ï¸ Error Handling

The system includes robust error handling for:
- Invalid image formats
- Network connectivity issues
- GPU memory limitations
- Translation service failures

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- **Salesforce**: For the BLIP model
- **Hugging Face**: For the Transformers library
- **Google**: For Translation and TTS services
- **Gradio**: For the interactive UI framework

---

â­ **Star this repo if you find it useful!**